{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11409170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/nasrin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/nasrin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.025*\"nation\" + 0.016*\"minor\" + 0.015*\"right\" + 0.012*\"liber\" + 0.011*\"state\" + 0.010*\"group\" + 0.010*\"polit\" + 0.009*\"cultur\" + 0.008*\"immigr\" + 0.008*\"peopl\"\n",
      "Topic: 1 \n",
      "Words: 0.001*\"nation\" + 0.001*\"minor\" + 0.001*\"liber\" + 0.001*\"state\" + 0.001*\"group\" + 0.001*\"polit\" + 0.001*\"right\" + 0.001*\"cultur\" + 0.001*\"peopl\" + 0.001*\"feder\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"nation\" + 0.007*\"liber\" + 0.007*\"minor\" + 0.006*\"right\" + 0.006*\"state\" + 0.006*\"cultur\" + 0.005*\"polit\" + 0.005*\"group\" + 0.005*\"immigr\" + 0.005*\"feder\"\n",
      "Topic: 3 \n",
      "Words: 0.001*\"right\" + 0.001*\"nation\" + 0.001*\"minor\" + 0.001*\"group\" + 0.001*\"liber\" + 0.001*\"state\" + 0.001*\"cultur\" + 0.001*\"polit\" + 0.000*\"peopl\" + 0.000*\"govern\"\n",
      "Topic: 4 \n",
      "Words: 0.002*\"nation\" + 0.001*\"right\" + 0.001*\"minor\" + 0.001*\"liber\" + 0.001*\"polit\" + 0.001*\"group\" + 0.001*\"cultur\" + 0.001*\"immigr\" + 0.001*\"govern\" + 0.001*\"state\"\n",
      "Topic: 5 \n",
      "Words: 0.002*\"nation\" + 0.001*\"minor\" + 0.001*\"right\" + 0.001*\"group\" + 0.001*\"liber\" + 0.001*\"state\" + 0.001*\"cultur\" + 0.001*\"polit\" + 0.001*\"immigr\" + 0.001*\"googl\"\n",
      "Topic: 6 \n",
      "Words: 0.001*\"nation\" + 0.001*\"right\" + 0.001*\"minor\" + 0.001*\"liber\" + 0.001*\"state\" + 0.001*\"group\" + 0.001*\"cultur\" + 0.001*\"govern\" + 0.001*\"polit\" + 0.001*\"immigr\"\n",
      "Topic: 7 \n",
      "Words: 0.002*\"nation\" + 0.001*\"right\" + 0.001*\"minor\" + 0.001*\"liber\" + 0.001*\"group\" + 0.001*\"polit\" + 0.001*\"state\" + 0.001*\"immigr\" + 0.001*\"cultur\" + 0.001*\"peopl\"\n",
      "Topic: 8 \n",
      "Words: 0.002*\"nation\" + 0.001*\"right\" + 0.001*\"minor\" + 0.001*\"liber\" + 0.001*\"polit\" + 0.001*\"peopl\" + 0.001*\"state\" + 0.001*\"googl\" + 0.001*\"group\" + 0.001*\"immigr\"\n",
      "Topic: 9 \n",
      "Words: 0.002*\"nation\" + 0.001*\"right\" + 0.001*\"minor\" + 0.001*\"state\" + 0.001*\"polit\" + 0.001*\"liber\" + 0.001*\"cultur\" + 0.001*\"group\" + 0.001*\"immigr\" + 0.001*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "# Ensure nltk tokenizer works correctly with UTF-8 encoding\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set file path\n",
    "file_path = '/Users/nasrin/Desktop/Multiculturalism_sylabes&research/Resourses/1/0.txt'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise Exception(f\"The file at the specified path does not exist: {file_path}\")\n",
    "\n",
    "# Read the content of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Preprocess the text\n",
    "processed_docs = preprocess(text_data)\n",
    "\n",
    "# Check if processed_docs is empty\n",
    "if not processed_docs:\n",
    "    raise ValueError(\"Preprocessed document is empty; check preprocessing steps.\")\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "# If there is very little text, no_below should be 1 to ensure we have enough words.\n",
    "dictionary = corpora.Dictionary([processed_docs])\n",
    "\n",
    "# Create a bag of words\n",
    "bow_corpus = [dictionary.doc2bow(processed_docs)]\n",
    "\n",
    "# Check if the BoW corpus is empty\n",
    "if not bow_corpus[0]:\n",
    "    raise ValueError(\"Bag of words corpus is empty; the document may be too short for LDA modeling.\")\n",
    "\n",
    "# Setting up LDA\n",
    "# With small text, we may want to reduce the number of topics.\n",
    "num_topics = min(10, len(dictionary))  # Number of topics should not exceed number of unique tokens\n",
    "if num_topics == 0:\n",
    "    raise ValueError(\"The number of unique tokens is zero; cannot model topics.\")\n",
    "\n",
    "lda_model = models.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=2)\n",
    "\n",
    "# Get the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb0238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
